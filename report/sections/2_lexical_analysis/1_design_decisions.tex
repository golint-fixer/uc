\subsection{Design Decisions}

The design of the lexer has been influenced by a set of core values, where consistency, simplicity and correctness have been considered fundamental. Several of the described design decisions arose from discussions on how to eliminate seeming inconsistencies, unnecessary complexities or non-determinisms within early and evolving implementations of the lexer.

% TODO: comments, source code positions, ...
% Big5 for Chinese, ISO-8859-1 for Western European languages.

% To allow for the common use case of writing source code in ASCII while having comments written in the encoding of the programmer's locale, allow for arbitrary data within comments. This will allow for ISO-8859-1 encoding and Big5 encoding Chinese characters; e.g.

% // Hej världen!
% /* 世界您好 */

\subsubsection{Character Encodings}

To allow for future extensions of the lexer to support C11, a set of design choices regarding the handling of character encodings have been made.

Firstly, the in-memory representation of lexemes should use an encoding compatible with the execution character set of the program (see §5.2.1.2 \cite{c11_spec}). For this purpose, UTF-8 has been deemed suitable since it contains the basic character set and encodes each of those characters with a single byte, as required by the C11 specification. Furthermore, using UTF-8 encoding enables support for comments written in several different languages and alphabets, as required by an international community of programmers.

Secondly, the lexer should support automatic transformation of several on-disk character encodings to UTF-8, as long as the source file encoding may be determined deterministically. This would make the lexer useful to a broader community of programmers, who may use other source file encodings than UTF-8, such as UTF-16 on Windows operating systems. Specific UTF-16 encodings (e.g. little or big endian) may be determined deterministically based on the presence of byte order marks (BOMs)\footnote{Add support for Unicode with BOM detection: \url{https://github.com/mewmew/uc/issues/17}}.

\subsubsection{Token Source File Positions}

An early version of the lexer tried to be cleaver and excluded known lexeme affixes from tokens (such as the \texttt{//} prefix of line comments, and the \texttt{/*} and \texttt{*/} affixes of block comments), in part to save memory. An evaluation of the implementation revealed several issues, including inconsistencies (e.g. apostrophes were not excluded from character literals), unnecessary complexity for little gain (i.e. potential memory saving), and arguably incorrectness (i.e. the affixes are part of lexemes and should not be excluded).

A solution to the aforementioned issues was discovered when discussing how to handle the tracking of source file positions for tokens. Since both line comments and block comments have the same \textit{token type}, namely \textit{comment}, there is no way to distinguish block comments from line comments in a stream of tokens once known lexeme affixes have been stripped. Therefore, it is not possible to determine the length of the original lexeme without additional knowledge, such as the start and end positions in the source file.

Had the length of a lexeme been determinable, the end position of a token may have been inferred implicitly, from the start position and lexeme length, rather than stored explicitly. The amount of memory saved from not storing end positions explicitly (i.e. the size of an integer) is greater than or equal to the amount saved from stripping known lexeme affixes. Therefore, it does not make sence to strip lexeme affixes. The lexer should always store the entire lexeme, which allows for the end position to be inferred implicity, and effectively solves each of the aforementionmed issues \footnote{Always include token affixes in lexemes: \url{https://github.com/mewmew/uc/issues/14}}. Furthermore, this approach is consistent with other lexers (e.g. the ones generated by Gocc), which is important as the compiler will include support for multiple lexer backends.

\subsubsection{lexer: Emit error tokens at the start source position of the invalid token}

% ref: https://github.com/mewmew/uc/issues/19

Emit error tokens with positions at the start of the invalid token, and continue lexing directly succeeding the initial character of the invalid token.


\begin{itemize}
	\item Return lexer to start of failing token to print error, continue lexing from following token.
	Sentiment: block comment errors, and potential later block token extention errors, will be easier to find by print out of start position of offending token
	\item The content in comments is not validated and may be of any charset, this is to allow for the common practice of using the current locale as character encoding in source file, and non-ASCII characters in comments.
\end{itemize}

\begin{verbatim}
'a'     // Ok
   ^ // Next token

'aa'x   // error
 ^ // Next token

'\a'    // error
 ^ // Next token

''              // error
 ^ // Next token

'\' // error
 ^ // Next token

"aa" // ok
    ^// Next token

"a\a"   //

"aa\" //

"aaEOF  // error


"\q/*" "\q*/"     // failing escape -> failing first quote,
 ^ // Next tokken // starting and ending block comment,
                  // starting new eventually failing quote
\end{verbatim}

\subsubsection{lexer: Lex type tokens as identifiers}

% ref: https://github.com/mewmew/uc/issues/21

Lex type tokens as identifiers to solve the lexer hack, similar to how Clang handles this issue.

\subsubsection{lexer: New line should not be part of line comment lexemes}

% ref: https://github.com/mewmew/uc/issues/22

Currently the Gocc generated lexer and the hand-written lexer do not agree on whether trailing new line characters should be part of line comment lexemes.

After discussing the manner and evaluating how other projects handle this issue (e.g. the scanner package in Go), we've decided to not include trailing new line characters in the lexeme of line comments.

% It remains an open question whether the Gocc lexer should be patched to conform to this behaviour.
