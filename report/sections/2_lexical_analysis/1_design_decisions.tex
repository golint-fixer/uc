\subsection{Design Decisions}

The design of the lexer has been influenced by a set of core values, where consistency, simplicity and correctness have been considered fundamental. Several of the described design decisions arose from discussions on how to eliminate seeming inconsistencies, unnecessary complexities or non-determinisms within early and evolving implementations of the lexer.

\subsubsection{Character Encodings}

To allow for future extensions of the lexer to support C11, a set of design choices regarding the handling of character encodings have been made.

Firstly, the in-memory representation of lexemes should use an encoding compatible with the execution character set of the program (see ยง5.2.1.2 \cite{c11_spec}). For this purpose, UTF-8 has been deemed suitable since it contains the basic character set and encodes each of those characters with a single byte, as required by the C11 specification. Furthermore, using UTF-8 encoding enables support for comments written in several different languages and alphabets, as required by an international community of programmers.

Secondly, the lexer should support automatic transformation of several on-disk character encodings to UTF-8, as long as the source file encoding may be determined deterministically. This would make the lexer useful to a broader community of programmers, who may use other source file encodings than UTF-8, such as UTF-16 on Windows operating systems. Specific UTF-16 encodings (e.g. little or big endian) may be determined deterministically based on the presence of byte order marks (BOMs)\footnote{Add support for Unicode with BOM detection: \url{https://github.com/mewmew/uc/issues/17}}.

\subsubsection{Token Source File Positions}

An early version of the lexer tried to be cleaver and excluded known lexeme affixes from tokens (such as the \texttt{//} prefix of line comments, and the \texttt{/*} and \texttt{*/} affixes of block comments), in part to save memory. An evaluation of the implementation revealed several issues, including inconsistencies (e.g. apostrophes were not excluded from character literals), unnecessary complexity for little gain (i.e. potential memory saving), and arguably incorrectness (i.e. the affixes are part of lexemes and should not be excluded).

A solution to the aforementioned issues was discovered when discussing how to handle the tracking of source file positions for tokens. Since both line comments and block comments have the same \textit{token type}, namely \textit{comment}, there is no way to distinguish block comments from line comments in a stream of tokens once known lexeme affixes have been stripped. Therefore, it is not possible to determine the length of the original lexeme without additional knowledge, such as the start and end positions in the source file.

Had the length of a lexeme been determinable, the end position of a token may have been inferred implicitly, from the start position and lexeme length, rather than stored explicitly. The amount of memory saved from not storing end positions explicitly (i.e. the size of an integer) is greater than or equal to the amount saved from stripping known lexeme affixes. Therefore, it does not make sense to strip lexeme affixes. The lexer should always store the entire lexeme, which allows for the end position to be inferred implicity, and effectively solves each of the aforementionmed issues\footnote{Always include token affixes in lexemes: \url{https://github.com/mewmew/uc/issues/14}}. Furthermore, this approach is consistent with other lexers (e.g. the ones generated by Gocc), which is important as the compiler will include support for multiple lexer backends.

\subsubsection{Error Handling}
\label{sec:error_handling}

The input to the lexer is often human-generated, and humans make mistakes, therefore fault tolerant lexing and informative error messages are essential. To keep error handling consistent, the lexer will always emit error tokens with positions at the start of invalid lexemes (in part to facilitate error reporting for unterminated block comments and string literals), and continue lexing directly succeding the initial character of the invalid lexeme\footnote{Emit errors at the start positions of invalid tokens: \url{https://github.com/mewmew/uc/issues/19}}, as illustrated in figure \ref{fig:invalid_lexemes}.

The content within comments is intentionally not validated and may be encoded in any charset (e.g. Big5 for Chinese, or ISO-8859-1 for Western European languages).

\begin{figure}[htbp]
	\centering
\begin{BVerbatim}
'a'    // ok
   ^   // start of next token

'aa'   // error: multi-byte character literal
 ^     // start of next token

'\q'   // error: unknown escape sequence
 ^     // start of next token

''     // error: unterminated character literal
 ^     // start of next token

'\'    // error: unknown escape sequence
 ^     // start of next token
\end{BVerbatim}
	\caption{Lexing of invalid lexemes.}
	\label{fig:invalid_lexemes}
\end{figure}

\subsubsection{The Lexer Hack}

To allow for future extensions of the lexer to support user-defined types, special precaution has been taken with regards to the \textit{typedef-name} problem, which has been described in great detail by Eli Bendersky \cite{typedef_name_problem}. In essence, the problem is that identifiers may not be distinguishable from user-defined types during lexical analysis, without additional context. An example by Eli is presented in listing \ref{lst:typedef_name_problem} which succinctly captures the context sensitivity of the problem, as \texttt{AA} on line 4 may refer to a user-defined character type or an integer identifier; therefore the size of \texttt{aa} is 1 byte, while the size of \texttt{bb} is the size of an integer.

\begin{lstlisting}[language=C,style=c,caption={An example of the \textit{typedef-name} problem.},label={lst:typedef_name_problem}]
typedef char AA;

void foo() {
	int aa = sizeof(AA), AA, bb = sizeof(AA);
}
\end{lstlisting}

To handle types consistently throughout the subsequent stages of the compiler, all type lexemes (e.g. \texttt{char}, \texttt{int} and \texttt{void}) are emitted as \textit{identifier} tokens during the lexical analysis. The type checker, which has access to additional context, of the semantic analysis stage will later annotate the abstract syntax tree to distingush between identifiers and types\footnote{Lex type tokens as identifiers: \url{https://github.com/mewmew/uc/issues/21}}. This idea was taken from how Clang handles \textit{the lexer hack} \cite{the_lexer_hack,clang_no_lexer_hack}.
